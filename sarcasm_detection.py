# -*- coding: utf-8 -*-
"""Sarcasm Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x64o2ZRyXYBwElLPaOgTnOs3PJkqsjrq

# Data Annotation Process:

1. *Data collection*: Collected 120 sentences from any source you find interesting (e.g., literature, Tweets, news articles, reviews, etc.)

2. *Task design*: Come up with a binary (i.e., only two labels) sentence-level classification task that you would like to perform on your sentences. Be creative, and make sure your task isn't too easy (e.g., perhaps the labels have some degree of subjectivity to them, or the task is otherwise complex for humans). Write up annotator guidelines/instructions on how you would like people to label your data.

3. On your dataset, collect annotations from **two** classmates for your task. Everyone in this class will need to both create their own dataset and also serve as an annotator for two other classmates. In order to get everything done on time, you need to complete the following steps:

> *   Find two classmates willing to label 120 sentences each (use the Piazza "search for teammates" thread if you're having issues finding labelers).
*   Send them your annotation guidelines and a way that they can easily annotate the data (e.g., a spreadsheet or Google form)
*   Collect the labeled data from each of the two annotators.
*   Sanity check the data for basic cleanliness (are all examples annotated? are all labels allowable ones?)

4. Collect feedback from annotators about the task including annotation time and obstacles encountered (e.g., maybe your guidelines were confusing! or maybe some sentences were particularly hard to annotate!)

5. Calculate and report inter-annotator agreement.

6. Aggregate output from both annotators to create final dataset.

7. Perform NLP experiments on your new dataset!

**Source of data:**  

I have collected the unlabeled text data from various sources, including web pages and also, some sentences were generated by ChatGPT.

**Why you chose it?**

Sarcasm is a crucial aspect of human communication, used to express irony, humor, and criticism. However, sarcasm can lead to misunderstandings and miscommunication, especially in online conversations where tone is not present. The ability to detect sarcasm is essential in natural language processing systems to provide appropriate responses. The motivation for sarcasm detection is to improve the accuracy and effectiveness of communication in various contexts, both in human-human and human-machine interactions. By detecting sarcasm, we can better understand the true meaning of a statement, leading to more effective communication and improved user experiences.


**What kind of sentence selection process used?**

For the sentence selection process, I tried to include sentences that were unambiguously sarcastic, as well as sentences that could be interpreted either sarcastically or literally depending on the context.

<!-- This helped to ensure that I was trained on a range of different types of sarcastic statements. -->

In addition to selecting a diverse range of sarcastic statements, I also included sentences with idiomatic phrases to increase the complexity of the sentences as this would help the model to get trained on a range of different types of linguistic structures that are commonly used in natural language.

Instead of aiming for a perfectly balanced dataset, I selected more sarcastic comments than non-sarcastic comments. This was done because in real-world situations, sarcastic comments are typically less frequent than non-sarcastic comments. By having a larger proportion of sarcastic comments in the dataset, I can ensure that the model would be trained on the types of sarcastic comments that are most likely to occur in real-world conversations.

**Text Classification task:**

Sarcasm detection is the task of identifying whether a statement is sarcastic or not.

### ANNOTATION GUIDELINES : ###

1.   The two categories for sarcasm detection task are: Sarcastic and Non-Sarcastic. If the sentence is sarcastic, the annotator needs to label it as "S" and if the sentence is non-sarcastic, the annotator needs to label it as "NS".

2.  Descriptions of the two catefories and representative examples of each category:

*   Sarcastic: The statement is meant to convey a meaning that is different from what is actually said, often using irony or humor to express the opposite.
<!-- Annotators should label a statement as sarcastic if they believe the speaker's intention is to convey a sarcastic meaning. -->

    Examples of sarcastic comments:

    "Thanks for letting me know about the traffic after I'm already stuck in it."

    "Oh sure, let's all take advice from the person who's never done this before."

<!-- In each of the above examples, the speaker's tone and context indicate that they actually dislike the situation they are describing, but are using sarcasm to express their frustration or annoyance. -->

*   Non-sarcastic:The statement is meant to be taken literally, and there is no hidden or ironic meaning intended.

    Examples of non-sarcastic comments:

    "I had a great time at the concert."

    "I'm so happy to be here with you."

<!-- Annotators should label a statement as non-sarcastic if they believe the speaker's intention is to convey a straightforward meaning. -->
    
<!-- In each of the above examples, the speaker's tone and context indicate that they are expressing a sincere and positive sentiment. There is no intended irony or hidden meaning. -->

3. Corner Cases


*   Irony and sarcasm can sometimes be confused as they both involve saying something opposite of the intended meaning. Irony, however, is often used to convey a tone or mood, while sarcasm is meant to be insulting or mocking.

    Example of irony: "Great, I forgot my umbrella on the one day it's actually raining." Here, the speaker is expressing the opposite of what they actually mean.

    Example of sarcasm: "Oh, sure, I love working late on a Friday night." Here, the speaker is using a tone that implies they are not happy about the situation.

*   Some statements can be interpreted in different ways depending on the context, tone, or perspective.
For example consider this statement, "I'm so glad we're stuck in traffic." This statement could be interpreted as sarcastic (i.e., the speaker is actually annoyed about being stuck in traffic), or as non-sarcastic (i.e., the speaker is genuinely happy to be in the car with the person they are speaking to).

*   Some statements may contain idiomatic phrases or expressions that are commonly used in sarcastic or non-sarcastic contexts. For example, "It's raining cats and dogs" means that it's raining heavily, not that actual cats and dogs are falling from the sky. These phrases can be used sarcastically or non-sarcastically depending on the context.

Feedback from annotators:
- Some of the sentences are ambiguous. Based on the context, those sentences can be either sarcastic or non-sarcastic.
- Labelling the idiomatic sentences is a bit challenging. The meaning of the idiom needs to be known.

Changes that can be made to the current setup based on feedback from annotators:
- Simplifying the categories by removing the ambiguity or irony vs sarcasm distinction could make it easier for annotators to make clear decisions.
- Providing the preceding and following sentences could help provide a better understanding of the context and improve the accuracy of the annotations.
- Ensuring that sentences are clear and easy to understand so that the annotators can understand the sentences and annotate better.

## Inter-Annotator agreement using Cohen's Kappa

*Cohen suggested the Kappa result be interpreted as follows: values ≤ 0 as indicating no agreement and 0.01–0.20 as none to slight, 0.21–0.40 as fair, 0.41– 0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1.00 as almost perfect agreement.*
"""

import torch
import random
import numpy as np

torch.manual_seed(0)
torch.cuda.manual_seed(0)
np.random.seed(0)
random.seed(0)

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

# %cd "/content/gdrive/MyDrive/UMass/Spring'23/CS685/HW1-Annotated Data"

import pandas as pd
from sklearn.metrics import cohen_kappa_score

# Load annotations
annotator1 = pd.read_csv('annotator1.csv')
annotator2 = pd.read_csv('annotator2.csv')

# Compute the raw agreement
raw_agreement = (annotator1['Label'] == annotator2['Label']).mean()
print("Raw agreement in percentage= {} ".format(raw_agreement*100))

# Compute Cohen's Kappa
kappa = cohen_kappa_score(annotator1['Label'], annotator2['Label'])
print("Cohen's Kappa = {:.3f}".format(kappa))

"""###*RAW AGREEMENT*: 95%
###*COHEN'S KAPPA*: 0.895

## *Aggregate* the annotations from both annotators
### AGGREGATION STRATEGY###

To resolve any ties in the annotation of the dataset, I incorporated a third annotated dataset that was labeled by me.

The following tie-breaking strategy was employed:
- If both the annotator's agreed on a label, that label was retained as the final label.
- If the annotator's disagreed, the label provided by the third dataset was used instead.
"""

final_dataset = pd.read_csv('final_dataset.csv')
print(final_dataset)

"""# Part 2: Text classification

Now we'll move onto fine-tuning  pretrained language models specifically on your dataset. This part of the homework is meant to be an introduction to the HuggingFace library, and it contains code that will potentially be useful for your final projects. Since we're dealing with large models, the first step is to change to a GPU runtime.

## Adding a hardware accelerator

Please go to the menu and add a GPU as follows:

`Edit > Notebook Settings > Hardware accelerator > (GPU)`

Run the following cell to confirm that the GPU is detected.
"""

## Adding a hardware accelerator

import torch

# Confirm that the GPU is detected

assert torch.cuda.is_available()

# Get the GPU device name.
device_name = torch.cuda.get_device_name()
n_gpu = torch.cuda.device_count()
print(f"Found device: {device_name}, n_gpu: {n_gpu}")
device = torch.device("cuda")

"""## Installing Hugging Face's Transformers library

"""

!pip install transformers
!pip install -U -q PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
print('success!')

import os
import zipfile

# Download helper functions file
helper_file = drive.CreateFile({'id': '16HW-z9Y1tM3gZ_vFpJAuwUDohz91Aac-'})
helper_file.GetContentFile('helpers.py')
print('helper file downloaded! (helpers.py)')

# Download sample file of tweets
data_file = drive.CreateFile({'id': '1QcoAmjOYRtsMX7njjQTYooIbJHPc6Ese'})
data_file.GetContentFile('tweets.csv')
print('sample tweets downloaded! (tweets.csv)')

"""The cell below imports some helper functions we wrote to demonstrate the task on the sample tweet dataset."""

from helpers import tokenize_and_format, flat_accuracy

"""# Part 1: Data Prep and Model Specifications

Tokenize and format the data as BERT requires.
"""

from helpers import tokenize_and_format, flat_accuracy
import pandas as pd

df = pd.read_csv('final_dataset.csv')
df['Label'].replace(['NS', 'S'], [0,1], inplace=True)
#df = pd.read_csv('tweets.csv')

df = df.sample(frac=1).reset_index(drop=True)

texts = df.Text.values
labels = df.Label.values

### tokenize_and_format() is a helper function provided in helpers.py ###
input_ids, attention_masks = tokenize_and_format(texts)

# Convert the lists into tensors.
input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(labels)

# Print sentence 0, now as a list of IDs.
print('Original: ', texts[0])
print('Token IDs:', input_ids[0])

"""## Create train/test/validation splits

Here we split your dataset into 3 parts: a training set, a validation set, and a testing set. Each item in your dataset will be a 3-tuple containing an input_id tensor, an attention_mask tensor, and a label tensor.


"""

total = len(df)

num_train = int(total * .8)
num_val = int(total * .1)
num_test = total - num_train - num_val

# make lists of 3-tuples (already shuffled the dataframe in cell above)

train_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train)]
val_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train, num_val+num_train)]
test_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_val + num_train, total)]

train_text = [texts[i] for i in range(num_train)]
val_text = [texts[i] for i in range(num_train, num_val+num_train)]
test_text = [texts[i] for i in range(num_val + num_train, total)]

"""I choose the model we want to finetune from https://huggingface.co/transformers/pretrained_models.html. As the task requires us to label sentences, we wil be using BertForSequenceClassification below."""

from transformers import BertForSequenceClassification, AdamW, BertConfig

model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased", # Use the 12-layer BERT model, with an uncased vocab.
    num_labels = 2, # The number of output labels.
    output_attentions = False, # Whether the model returns attentions weights.
    output_hidden_states = False, # Whether the model returns all hidden-states.
)

# Tell pytorch to run this model on the GPU.
model.cuda()

"""# Fine-tune the model

"""

#import numpy as np
# function to get validation accuracy
def get_validation_performance(val_set):
    # Put the model in evaluation mode
    model.eval()

    # Tracking variables
    total_eval_accuracy = 0
    total_eval_loss = 0

    num_batches = int(len(val_set)/batch_size) + 1

    total_correct = 0

    for i in range(num_batches):

      end_index = min(batch_size * (i+1), len(val_set))

      batch = val_set[i*batch_size:end_index]

      if len(batch) == 0: continue

      input_id_tensors = torch.stack([data[0] for data in batch])
      input_mask_tensors = torch.stack([data[1] for data in batch])
      label_tensors = torch.stack([data[2] for data in batch])

      # Move tensors to the GPU
      b_input_ids = input_id_tensors.to(device)
      b_input_mask = input_mask_tensors.to(device)
      b_labels = label_tensors.to(device)

      # Tell pytorch not to bother with constructing the compute graph during
      # the forward pass, since this is only needed for backprop (training).
      with torch.no_grad():

        # Forward pass, calculate logit predictions.
        outputs = model(b_input_ids,
                                token_type_ids=None,
                                attention_mask=b_input_mask,
                                labels=b_labels)
        loss = outputs.loss
        logits = outputs.logits

        # Accumulate the validation loss.
        total_eval_loss += loss.item()

        # Move logits and labels to CPU
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # Calculate the number of correctly labeled examples in batch
        pred_flat = np.argmax(logits, axis=1).flatten()
        labels_flat = label_ids.flatten()
        num_correct = np.sum(pred_flat == labels_flat)
        total_correct += num_correct

    # Report the final accuracy for this validation run.
    avg_val_accuracy = total_correct / len(val_set)
    return avg_val_accuracy

"""##### Hyperparameter tuning"""

learning_rate = np.array([1e-6, 1e-5, 5e-5, 1e-4, 5e-4])
num_epochs = np.array([5, 10, 15, 20, 25])
batchsize = np.array([4, 8, 12, 16, 32])
epsilon = np.array([1e-8, 5e-8, 1e-7, 5e-7, 1e-6])
best_val_acc = 0
best_set= 0


for i in range(0,len(learning_rate)):
  batch_size = batchsize[i]
  optimizer = AdamW(model.parameters(),
                  lr = learning_rate[i],
                  eps = epsilon[i]
                )
  epochs = num_epochs[i]
  s = i

  print("============== Set {:} ================".format(i+1))

  for epoch_i in range(0, epochs):
    # Perform one full pass over the training set.

    # Reset the total loss for this epoch.
    total_train_loss = 0

    # Put the model into training mode.
    model.train()

    # For each batch of training data...
    num_batches = int(len(train_set)/batch_size) + 1

    for z in range(num_batches):
      end_index = min(batch_size * (z+1), len(train_set))

      batch = train_set[z*batch_size:end_index]

      if len(batch) == 0: continue

      input_id_tensors = torch.stack([data[0] for data in batch])
      input_mask_tensors = torch.stack([data[1] for data in batch])
      label_tensors = torch.stack([data[2] for data in batch])

      # Move tensors to the GPU
      b_input_ids = input_id_tensors.to(device)
      b_input_mask = input_mask_tensors.to(device)
      b_labels = label_tensors.to(device)

      # Clear the previously calculated gradient
      model.zero_grad()

      # Perform a forward pass (evaluate the model on this training batch).
      outputs = model(b_input_ids,
                            token_type_ids=None,
                            attention_mask=b_input_mask,
                            labels=b_labels)
      loss = outputs.loss
      logits = outputs.logits

      total_train_loss += loss.item()

      # Perform a backward pass to calculate the gradients.
      loss.backward()

      # Update parameters and take a step using the computed gradient.
      optimizer.step()

  print(f"Total loss: {total_train_loss}")
  val_acc = get_validation_performance(val_set)
  print(f"Validation accuracy: {val_acc}")
  if(val_acc>best_val_acc):
    best_val_acc = val_acc
    best_set = s

print("\n")
print("=============================================================")
print(f"Best Validation accuracy: {best_val_acc}")
print("\n")
print(f"Best Parameters  \n Learning rate:{learning_rate[best_set]}, Epochs:{num_epochs[best_set]}, Batch Size: {batchsize[best_set]}, Epsilon: {epsilon[best_set]}")

# The following parameters yielded highest validation accuracy:

batch_size = batchsize[best_set]
optimizer = AdamW(model.parameters(),
                  lr = learning_rate[best_set],
                  eps = epsilon[best_set]
                )
epochs = num_epochs[best_set]

#import random

# training loop

# For each epoch...
for epoch_i in range(0, epochs):
    # Perform one full pass over the training set.

    print("")
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    print('Training...')

    # Reset the total loss for this epoch.
    total_train_loss = 0

    # Put the model into training mode.
    model.train()

    # For each batch of training data...
    num_batches = int(len(train_set)/batch_size) + 1

    for i in range(num_batches):
      end_index = min(batch_size * (i+1), len(train_set))

      batch = train_set[i*batch_size:end_index]

      if len(batch) == 0: continue

      input_id_tensors = torch.stack([data[0] for data in batch])
      input_mask_tensors = torch.stack([data[1] for data in batch])
      label_tensors = torch.stack([data[2] for data in batch])

      # Move tensors to the GPU
      b_input_ids = input_id_tensors.to(device)
      b_input_mask = input_mask_tensors.to(device)
      b_labels = label_tensors.to(device)

      # Clear the previously calculated gradient
      model.zero_grad()

      # Perform a forward pass (evaluate the model on this training batch).
      outputs = model(b_input_ids,
                            token_type_ids=None,
                            attention_mask=b_input_mask,
                            labels=b_labels)
      loss = outputs.loss
      logits = outputs.logits

      total_train_loss += loss.item()

      # Perform a backward pass to calculate the gradients.
      loss.backward()

      # Update parameters and take a step using the computed gradient.
      optimizer.step()

    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set. Implement this function in the cell above.
    print(f"Total loss: {total_train_loss}")
    val_acc = get_validation_performance(val_set)
    print(f"Validation accuracy: {val_acc}")

print("")
print("Training complete!")

"""# Evaluate the model on the test set


"""

get_validation_performance(test_set)

"""## Hyperparameter Selection Process

I have employed a random selection process to determine the optimal values for the learning rate, batch size, number of epochs, and epsilon hyperparameters. In order to optimize the time required to run the model, rather than iterating through all potential values of each hyperparameter, I have passed them as a collective to the model. The model then computes the validation accuracy for each distinct set of hyperparameter values, and the set which results in the highest accuracy is selected as the optimal set. This optimal set of hyperparameters is then used to evaluate the model on the test set.

Regarding the discrepancy between validation and test accuracy, it appears that your model is performing better on the test set than on the validation set. One possible explanation for the difference in accuracy could be that the validation set may not be representative of the data distribution or has different characteristics than the test set. Another possibility is that the model may have been trained on too few examples and could benefit from additional training data. If more training data is given to the model, then it can gain more insights and it can perform better on validation and test set.

## Error analysis of the model
"""

## YOUR ERROR ANALYSIS CODE HERE
## print out up to 5 test set examples (or adversarial examples) that your model gets wrong

model.eval()
#batch_size=16
#print(len(test_set))
input_id_tensors = torch.stack([test[0] for test in test_set])
input_mask_tensors = torch.stack([test[1] for test in test_set])
label_tensors = torch.stack([test[2] for test in test_set])

input_ids = input_id_tensors.to(device)
input_mask = input_mask_tensors.to(device)
labels = label_tensors.to(device)

with torch.no_grad():
    # Forward pass, calculate logit predictions.
    outputs = model(input_ids,
                            token_type_ids=None,
                            attention_mask=input_mask,
                            labels=labels)
    #print(outputs)
    logits = outputs.logits

    # Move logits and labels to CPU
    logits = logits.detach().cpu().numpy()
    label_ids = labels.to('cpu').numpy()

    # Calculate the number of correctly labeled examples in batch
    pred_flat = np.argmax(logits, axis=1).flatten()
    labels_flat = label_ids.flatten()
    print(pred_flat)
    print(labels_flat)

for i in range(len(pred_flat)):
  if pred_flat[i] != labels_flat[i]:
    print(f"Sentence : {test_text[i]}")

adversarial_examples = ["The project is a hot potato, and no one wants to take responsibility for it.",
                        "The new policy is a bitter pill to swallow, but it's necessary for the company's long-term success.",
                        "Sure, I'd love to work overtime for no extra pay, said no one ever.",
                        "Well, isn't this just the cherry on top of the dumpster fire that is my life?",
                        "Oh, joy. Another conference call where we talk in circles and accomplish nothing." ,
                        "Oh, great. Another meeting to discuss the meeting we just had about the meeting we're going to have next week." ,
                        "Congratulations, you just broke the record for the slowest response time ever." ,
                        "Oh, fantastic. I get to work on a Saturday.",
                        "He's barking up the wrong tree."]
ae_true_label = [0, 0, 1, 1, 1, 1, 1, 1, 0]

### tokenize_and_format() is a helper function provided in helpers.py ###
ae_input_ids, ae_attention_masks = tokenize_and_format(adversarial_examples)

model.eval()

# Convert the lists into tensors.
ae_input_ids = torch.cat(ae_input_ids, dim=0)
ae_attention_masks = torch.cat(ae_attention_masks, dim=0)
ae_labels = torch.tensor(ae_true_label)

 # Move tensors to the GPU
b_input_ids = ae_input_ids.to(device)
b_input_mask = ae_attention_masks.to(device)
b_labels = ae_labels.to(device)

ae_test_set = [(ae_input_ids[i], ae_attention_masks[i], ae_labels[i]) for i in range(len(adversarial_examples))]


outputs = model(b_input_ids,
                token_type_ids=None,
                attention_mask=b_input_mask,
                labels=b_labels)
logits = outputs.logits

# Move logits and labels to CPU
logits = logits.detach().cpu().numpy()
label_ids = b_labels.to('cpu').numpy()

# Calculate the number of correctly labeled examples in batch
pred_flat = np.argmax(logits, axis=1).flatten()
labels_flat = label_ids.flatten()

for i in range(len(pred_flat)):
  if pred_flat[i] != labels_flat[i]:
    print(f"Sentence : {adversarial_examples[i]}")

"""### QUALITATIVE ANALYSIS OF THE ABOVE EXAMPLES

- The model is still having trouble comprehending idiomatic phrases. To tackle this issue, we can provide the model with more examples of idiomatic phrases and sarcastic sentences to learn from, which can help improve its ability to classify such sentences accurately.
- The model it is not capturing the sarcasm conveyed through the use of the words "great", and is incorrectly labelling them as non-sarcastic. This might be due to the model's inability to recognize the context in which these words are used, as well as the overall tone of the sentence.
"""